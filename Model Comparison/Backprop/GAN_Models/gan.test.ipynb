{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the image shape  (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Adjust sys.argv to remove unwanted Jupyter arguments\n",
    "sys.argv = sys.argv[:1]  # Keep only the script name, remove Jupyter's arguments\n",
    "\n",
    "# Now proceed with argparse as usual\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=200, help=\"size of the batches\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of CPU threads for data loading\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=2000, help=\"interval between image sampling\")\n",
    "\n",
    "# Parse the arguments\n",
    "opt = parser.parse_args()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "print(\"this is the image shape \", img_shape)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]  \n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loading Classes and Functions\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, dataX, dataY=None):\n",
    "        self.dataX = np.load(dataX)\n",
    "        self.dataY = np.load(dataY) if dataY is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataX)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.dataX[idx], dtype=torch.float32)\n",
    "        label = (\n",
    "            torch.tensor(self.dataY[idx], dtype=torch.long)\n",
    "            if self.dataY is not None\n",
    "            else None\n",
    "        )\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "dataX = \"../../../data/mnist/trainX.npy\"\n",
    "dataY = \"../../../data/mnist/trainY.npy\"\n",
    "devX = \"../../../data/mnist/validX.npy\"\n",
    "devY = \"../../../data/mnist/validY.npy\"\n",
    "testX = \"../../../data/mnist/testX.npy\"\n",
    "testY = \"../../../data/mnist/testY.npy\"\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = NumpyDataset(dataX, dataY)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "dev_dataset = NumpyDataset(devX, devY)\n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=200, shuffle=False)\n",
    "\n",
    "test_dataset = NumpyDataset(testX, testY)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=200, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Encoder (Same as VAE) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=360, latent_dim=20):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)  # Hidden layer 2 to mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # Hidden layer 2 to log-variance\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, sigma=0.02):\n",
    "        for layer in [self.fc1, self.fc2, self.fc_mu, self.fc_logvar]:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=sigma)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# --- Decoder (Generator) ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=360, output_dim=784):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, sigma=0.02):\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=sigma)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = torch.sigmoid(self.fc3(z))  # Sigmoid activation in the output layer\n",
    "        return z\n",
    "\n",
    "\n",
    "# --- Discriminator (GAN) ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=360):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  # Output a single value for real/fake classification\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, sigma=0.02):\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=sigma)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid activation for binary output (real/fake)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- GAN-AE Model ---\n",
    "class GANAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=360, latent_dim=20,l2_lambda=1e-3):\n",
    "        super(GANAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "        self.discriminator = Discriminator(input_dim, hidden_dim)\n",
    "        self.l2_lambda = l2_lambda\n",
    "    def compute_l2_penalty(self):\n",
    "        l2_penalty = 0\n",
    "        for param in self.decoder.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_penalty += torch.sum(param**2)\n",
    "        return self.l2_lambda * l2_penalty\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss Functions ---\n",
    "def bce_loss(pred, target):\n",
    "    return F.binary_cross_entropy(pred, target,reduction=\"sum\")\n",
    "\n",
    "def gan_ae_loss(recon_x, x, real_or_fake, real_label=1, fake_label=0):\n",
    "    # Reconstruction loss\n",
    "    recon_loss =bce_loss(recon_x, x)\n",
    "\n",
    "    # Discriminator loss (binary cross entropy)\n",
    "    d_loss_real = bce_loss(real_or_fake, torch.full_like(real_or_fake, real_label))\n",
    "    d_loss_fake = bce_loss(real_or_fake, torch.full_like(real_or_fake, fake_label))\n",
    "\n",
    "    return recon_loss + d_loss_real + d_loss_fake\n",
    "\n",
    "def autoencoder_gradients(recon_x, x):\n",
    "    \"\"\"\n",
    "    Equation (4) - Autoencoder gradients\n",
    "    \"\"\"\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "def generator_gradients(discriminator_output):\n",
    "    \"\"\"\n",
    "    Equation (5) - Generator gradients\n",
    "    \"\"\"\n",
    "    return -torch.mean(torch.log(discriminator_output + 1e-8))\n",
    "\n",
    "def discriminator_gradients(discriminator_output_real, discriminator_output_fake):\n",
    "    \"\"\"\n",
    "    Equation (6) - Discriminator gradients\n",
    "    \"\"\"\n",
    "    return -torch.mean(torch.log(discriminator_output_real + 1e-8) + \n",
    "                      torch.log(1 - discriminator_output_fake + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 39297.0950625\n",
      "Epoch 1 Loss 26154.9494765625\n",
      "Epoch 2 Loss 21748.662234375\n",
      "Epoch 3 Loss 19286.81178125\n",
      "Epoch 4 Loss 17801.5094765625\n",
      "Epoch 5 Loss 16676.43815234375\n",
      "Epoch 6 Loss 15857.13468359375\n",
      "Epoch 7 Loss 15368.74139453125\n",
      "Epoch 8 Loss 15008.49491796875\n",
      "Epoch 9 Loss 14723.666\n",
      "Epoch 10 Loss 14492.8149296875\n",
      "Epoch 11 Loss 14321.977890625\n",
      "Epoch 12 Loss 14163.19466015625\n",
      "Epoch 13 Loss 14015.21926953125\n",
      "Epoch 14 Loss 13891.94984375\n",
      "Epoch 15 Loss 13806.196984375\n",
      "Epoch 16 Loss 13708.88112890625\n",
      "Epoch 17 Loss 13650.2826796875\n",
      "Epoch 18 Loss 13601.386671875\n",
      "Epoch 19 Loss 13541.22569921875\n",
      "Epoch 20 Loss 13486.61441015625\n",
      "Epoch 21 Loss 13445.2815078125\n",
      "Epoch 22 Loss 13424.01158984375\n",
      "Epoch 23 Loss 13367.80201953125\n",
      "Epoch 24 Loss 13348.44544921875\n",
      "Epoch 25 Loss 13297.1557421875\n",
      "Epoch 26 Loss 13296.330359375\n",
      "Epoch 27 Loss 13251.82384375\n",
      "Epoch 28 Loss 13231.8799609375\n",
      "Epoch 29 Loss 13211.09778515625\n",
      "Epoch 30 Loss 13188.4995390625\n",
      "Epoch 31 Loss 13171.61262109375\n",
      "Epoch 32 Loss 13153.49954296875\n",
      "Epoch 33 Loss 13130.82658203125\n",
      "Epoch 34 Loss 13121.4360078125\n",
      "Epoch 35 Loss 13096.86940234375\n",
      "Epoch 36 Loss 13083.21393359375\n",
      "Epoch 37 Loss 13063.15016015625\n",
      "Epoch 38 Loss 13057.48825\n",
      "Epoch 39 Loss 13038.4714140625\n",
      "Epoch 40 Loss 13031.4737734375\n",
      "Epoch 41 Loss 13029.08417578125\n",
      "Epoch 42 Loss 13016.01958984375\n",
      "Epoch 43 Loss 12994.8721015625\n",
      "Epoch 44 Loss 12981.8023828125\n",
      "Epoch 45 Loss 12980.49144921875\n",
      "Epoch 46 Loss 12962.12483203125\n",
      "Epoch 47 Loss 12958.2665234375\n",
      "Epoch 48 Loss 12941.17139453125\n",
      "Epoch 49 Loss 12933.70541015625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs=50# Training loop\n",
    "ganae = GANAE()\n",
    "optimizer_gen = torch.optim.Adam(ganae.parameters(), lr=0.002)\n",
    "optimizer_disc = torch.optim.Adam(ganae.discriminator.parameters(), lr=0.002)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        batch_size = data.size(0)\n",
    "        data = data.view(batch_size, -1)\n",
    "        \n",
    "        # Step 1: Train Autoencoder\n",
    "        optimizer_gen.zero_grad()\n",
    "        \n",
    "        # Get encoder outputs\n",
    "        mu, logvar = ganae.encoder(data)\n",
    "        z = ganae.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Generate reconstruction\n",
    "        recon_x = ganae.decoder(z)\n",
    "        \n",
    "        # Get discriminator predictions\n",
    "        disc_output = ganae.discriminator(recon_x)\n",
    "        \n",
    "        # Calculate autoencoder gradients (eq. 4)\n",
    "        ae_loss = autoencoder_gradients(recon_x, data)\n",
    "        \n",
    "        # Calculate generator gradients (eq. 5)\n",
    "        gen_loss = generator_gradients(disc_output)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_gen_loss = ae_loss + gen_loss\n",
    "        total_gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        # Step 2: Train Discriminator\n",
    "        optimizer_disc.zero_grad()\n",
    "        \n",
    "        # Generate new samples for discriminator training\n",
    "        z_fake = ganae.reparameterize(mu, logvar)\n",
    "        fake_samples = ganae.decoder(z_fake)\n",
    "        \n",
    "        # Get discriminator predictions for real and fake samples\n",
    "        d_real = ganae.discriminator(data)\n",
    "        d_fake = ganae.discriminator(fake_samples.detach())\n",
    "        \n",
    "        # Calculate discriminator loss (eq. 6)\n",
    "        d_loss = discriminator_gradients(d_real, d_fake)\n",
    "        \n",
    "        loss = gan_ae_loss(recon_x, data, disc_output)\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_disc.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Logging\n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     print(f\"Epoch [{epoch}/{num_epochs}] Batch [{batch_idx}/{len(train_loader)}] \"\n",
    "        #           f\"AE Loss: {ae_loss.item():.4f} \"\n",
    "        #           f\"Gen Loss: {gen_loss.item():.4f} \"\n",
    "        #           f\"Disc Loss: {d_loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch} Loss {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7246/2405283806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_bce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_mse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcalculate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mganae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7246/2405283806.py\u001b[0m in \u001b[0;36mcalculate_error\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mrecon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mrecon_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Calculate error\n",
    "def calculate_error(model, loader):\n",
    "    model.eval()\n",
    "    total_bce = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            data = data.view(data.size(0), -1)\n",
    "            recon_data, _ = model(data)\n",
    "            recon_data = recon_data.view(data.size(0), -1)\n",
    "            bce = F.binary_cross_entropy(recon_data, data, reduction='sum')\n",
    "            total_bce += bce.item()\n",
    "            mse = F.mse_loss(recon_data, data, reduction='sum')\n",
    "            total_mse += mse.item()\n",
    "            total_samples += data.size(0)\n",
    "\n",
    "    avg_bce = total_bce / total_samples\n",
    "    avg_mse = total_mse / total_samples\n",
    "    print(f\"BCE: {avg_bce:.4f}, MSE: {avg_mse:.4f}\")\n",
    "    return avg_bce, avg_mse\n",
    "\n",
    "calculate_error(ganae, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_gan_ae(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate classification error using the trained GAN-AE model and a logistic regression classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GAN-AE model with an encoder.\n",
    "    - loader: DataLoader providing batches of (data, labels).\n",
    "\n",
    "    Returns:\n",
    "    - err: Classification error percentage.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latents, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, label in loader:\n",
    "            # Flatten data and move to the appropriate device\n",
    "            data = data.view(data.size(0), -1).to(next(model.parameters()).device)\n",
    "            label = label.to(next(model.parameters()).device)  # Ensure labels are also on the same device\n",
    "\n",
    "            if label is not None:  # Collect latent representations and labels\n",
    "                z = model.encoder(data)\n",
    "                latents.append(z.cpu().numpy())\n",
    "                labels.append(label.cpu().numpy())\n",
    "    \n",
    "    if not labels:  # Handle case where no labels are provided\n",
    "        print(\"No labels provided for classification.\")\n",
    "        return None\n",
    "\n",
    "    # Stack latents and labels into arrays\n",
    "    latents = np.vstack(latents)\n",
    "    labels = np.hstack(labels).reshape(-1)\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "    print(f\"latents shape: {latents.shape}\")\n",
    "    # Ensure consistent lengths of latents and labels\n",
    "    min_len = min(len(latents), len(labels))\n",
    "    latents = latents[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "\n",
    "    # Split latent data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(latents, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train logistic regression on training split\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test split\n",
    "    predictions = clf.predict(X_test)\n",
    "    err = 100 * (1 - accuracy_score(y_test, predictions))\n",
    "    print(f\"Classification Error: {err:.2f}%\")\n",
    "    \n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7246/1984370151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_classification_gan_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mganae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7246/136267673.py\u001b[0m in \u001b[0;36mevaluate_classification_gan_ae\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Collect latent representations and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mlatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "evaluate_classification_gan_ae(ganae,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_masked_mse(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate the Masked Mean Squared Error (M-MSE) for a given model on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - loader: DataLoader providing batches of test data.\n",
    "\n",
    "    Returns:\n",
    "    - avg_mse: The average masked MSE over the entire dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_mse = 0.0  # Accumulate total masked MSE\n",
    "    total_samples = 0  # Total number of samples processed\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for data, _ in loader:\n",
    "            # Flatten images into vectors (batch_size, D)\n",
    "            data = data.view(data.size(0), -1)  # Shape: (batch_size, D)\n",
    "\n",
    "            # Create a binary mask: Half of the columns are masked\n",
    "            mask = torch.ones_like(data, dtype=torch.bool)  # Shape: (batch_size, D)\n",
    "            mask[:, : data.size(1) // 2] = 0  # Mask the first half of the columns\n",
    "\n",
    "            # Apply the mask to the input data\n",
    "            masked_data = data * mask.float()  # Zero out the masked part\n",
    "            \n",
    "            # Forward pass through the model to get the reconstructed images\n",
    "            output = model(masked_data)  # Assumes model outputs reconstructed data\n",
    "            if isinstance(output, tuple):  # Extract reconstructed images if output is a tuple\n",
    "                reconstructed = output[0]\n",
    "            else:\n",
    "                reconstructed = output\n",
    "\n",
    "            # Ensure proper shape of reconstructed data\n",
    "            reconstructed = reconstructed.view(data.size(0), -1)  # Shape: (batch_size, D)\n",
    "\n",
    "            # Calculate the M-MSE for the current batch\n",
    "            mse_batch = masked_mse(data, reconstructed, mask)  # Masked MSE for this batch\n",
    "            \n",
    "            # Accumulate results\n",
    "            total_mse += mse_batch.item() * data.size(0)  # Multiply by batch size\n",
    "            total_samples += data.size(0)  # Update total sample count\n",
    "\n",
    "    # Compute the final average M-MSE across the dataset\n",
    "    avg_mse = total_mse / total_samples\n",
    "    print(f\"Average Masked MSE: {avg_mse:.4f}\")\n",
    "\n",
    "    return avg_mse\n",
    "\n",
    "\n",
    "# Helper function for batch-level Masked MSE\n",
    "def masked_mse(x, x_hat, mask):\n",
    "    \"\"\"\n",
    "    Compute the Masked Mean Squared Error (M-MSE) for a single batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Original images (batch_size, D), where D is the number of pixels in each image.\n",
    "    - x_hat: Reconstructed images (batch_size, D).\n",
    "    - mask: Binary mask (batch_size, D), where 1 indicates unmasked pixels and 0 indicates masked pixels.\n",
    "\n",
    "    Returns:\n",
    "    - m_mse: Masked mean squared error for the batch.\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    error = (x - x_hat) ** 2\n",
    "    # Apply mask to focus only on masked-out regions\n",
    "    masked_error = error * (1 - mask.float())  \n",
    "\n",
    "    # Compute normalized MSE for the masked regions\n",
    "    batch_mse = masked_error.sum(dim=1) / (1 - mask.float()).sum(dim=1)\n",
    "\n",
    "    # Return average masked MSE for the batch\n",
    "    return batch_mse.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Masked MSE: 19.4237\n",
      "19.423651809692384\n"
     ]
    }
   ],
   "source": [
    "avg_mse = evaluate_masked_mse(ganae, test_loader)\n",
    "print(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the parent directory of GAN_Models to sys.path\n",
    "sys.path.append(os.path.abspath(\"../../\"))  # Adjust the path as necessary\n",
    "\n",
    "from density.fit_gmm import fit_gmm\n",
    "from density.eval_logpx import evaluate_logpx\n",
    "def evaluate_density_model(model,train_loader, test_loader):\n",
    "    gmm = fit_gmm(model=model,data_loader=train_loader,latent_dim=20)\n",
    "\n",
    "    log_px = evaluate_logpx(model=model,gmm=gmm,data_loader=test_loader,latent_dim=20)\n",
    "\n",
    "    print(f\"Log-likelihood: {log_px:.4f}\")\n",
    "    return log_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_density_model() missing 1 required positional argument: 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14569/2033792785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_density_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mganae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_density_model() missing 1 required positional argument: 'test_loader'"
     ]
    }
   ],
   "source": [
    "evaluate_density_model(ganae,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from utils.calc_perc_error import evaluate_perc_err\n",
    "def evaluate_model(model, train_loader, test_loader, latent_dim, n_components=75, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Evaluate the model on various metrics including BCE loss, classification error, GMM fitting, \n",
    "    Monte Carlo log-likelihood, and inference time.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        train_loader: DataLoader providing the training dataset.\n",
    "        test_loader: DataLoader providing the test dataset.\n",
    "        latent_dim: Dimension of the latent space.\n",
    "        n_components: Number of GMM components for fitting.\n",
    "        num_samples: Number of Monte Carlo samples for log-likelihood estimation.\n",
    "\n",
    "    Returns:\n",
    "        results: A dictionary containing evaluation metrics and total inference time.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting model evaluation...\")\n",
    "    inference_start_time = time.time()\n",
    "\n",
    "    results = {}\n",
    "    model.eval()\n",
    "\n",
    "    logging.info(\"Calculating Binary Cross-Entropy (BCE) loss...\")\n",
    "    bce_losses = []\n",
    "\n",
    "   \n",
    "    # logging.info(\"Evaluating classification error (%Err)...\")\n",
    "    # err = evaluate_perc_err(model, train_loader, test_loader)\n",
    "    # results['Classification_Error'] = err\n",
    "    # logging.info(f\"Classification error: {err:.4f}%\")\n",
    "\n",
    "    logging.info(\"Fitting GMM on latent space...\")\n",
    "    gmm = fit_gmm(train_loader, model, latent_dim=latent_dim, n_components=n_components)\n",
    "    logging.info(\"Finished fitting GMM.\")\n",
    "\n",
    "    logging.info(\"Evaluating Monte Carlo log-likelihood...\")\n",
    "    test_logpx = evaluate_logpx(test_loader, model, gmm, latent_dim=latent_dim, num_samples=num_samples)\n",
    "    results['Monte_Carlo_Log_Likelihood'] = test_logpx\n",
    "    logging.info(f\"Monte Carlo log-likelihood: {test_logpx:.4f}\")\n",
    "\n",
    "    total_inference_time = time.time() - inference_start_time\n",
    "    results['Total_Inference_Time'] = total_inference_time\n",
    "    logging.info(f\"Total inference time: {total_inference_time:.2f} seconds\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:24:19,386 - INFO - Starting model evaluation...\n",
      "2025-01-20 12:24:19,388 - INFO - Calculating Binary Cross-Entropy (BCE) loss...\n",
      "2025-01-20 12:24:19,389 - INFO - Fitting GMM on latent space...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting latent vectors from dataLoader using the provided model...\n",
      "Collected latent data shape: torch.Size([50000, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:27:29,410 - INFO - Finished fitting GMM.\n",
      "2025-01-20 12:27:29,411 - INFO - Evaluating Monte Carlo log-likelihood...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GMM to file: gmm.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:42:27,152 - INFO - Monte Carlo log-likelihood: -276.2071\n",
      "2025-01-20 12:42:27,153 - INFO - Total inference time: 1087.77 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Monte_Carlo_Log_Likelihood': -276.20710555114744,\n",
       " 'Total_Inference_Time': 1087.7655725479126}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(train_loader=train_loader,test_loader=test_loader,latent_dim=20,model=ganae,n_components=75,num_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
