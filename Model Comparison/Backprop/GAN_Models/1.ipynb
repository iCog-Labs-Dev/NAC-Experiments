{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the image shape  (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [Your provided setup code for argparse and data loading remains the same]\n",
    "# Adjust sys.argv to remove unwanted Jupyter arguments\n",
    "sys.argv = sys.argv[:1]  # Keep only the script name, remove Jupyter's arguments\n",
    "\n",
    "# Now proceed with argparse as usual\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=200, help=\"size of the batches\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of CPU threads for data loading\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=2000, help=\"interval between image sampling\")\n",
    "\n",
    "# Parse the arguments\n",
    "opt = parser.parse_args()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "print(\"this is the image shape \", img_shape)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]  \n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Data Loading Classes and Functions\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, dataX, dataY=None):\n",
    "        self.dataX = np.load(dataX)\n",
    "        self.dataY = np.load(dataY) if dataY is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataX)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.dataX[idx], dtype=torch.float32)\n",
    "        label = (\n",
    "            torch.tensor(self.dataY[idx], dtype=torch.long)\n",
    "            if self.dataY is not None\n",
    "            else None\n",
    "        )\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define file paths\n",
    "dataX = \"../../../data/mnist/trainX.npy\"\n",
    "dataY = \"../../../data/mnist/trainY.npy\"\n",
    "devX = \"../../../data/mnist/validX.npy\"\n",
    "devY = \"../../../data/mnist/validY.npy\"\n",
    "testX = \"../../../data/mnist/testX.npy\"\n",
    "testY = \"../../../data/mnist/testY.npy\"\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = NumpyDataset(dataX, dataY)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "dev_dataset = NumpyDataset(devX, devY)\n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=200, shuffle=False)\n",
    "\n",
    "test_dataset = NumpyDataset(testX, testY)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=200, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=20):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.fc_mean = nn.Linear(hidden_dim//2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim//2, latent_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        mean = self.fc_mean(h2)\n",
    "        logvar = self.fc_logvar(h2)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=512, output_dim=784):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim//2)\n",
    "        self.fc2 = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = torch.sigmoid(self.fc3(h2))\n",
    "        return output#.view(-1, opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=512):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.fc3 = nn.Linear(hidden_dim//2, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return torch.sigmoid(self.fc3(h2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(x, x_recon):\n",
    "    return F.binary_cross_entropy_with_logits(x_recon, x, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize models, optimizers, and loss functions\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=0.002)\n",
    "optimizer_D = torch.optim.Adam(decoder.parameters(), lr=0.002, weight_decay=1e-5)  # L2 regularization\n",
    "optimizer_Disc = torch.optim.Adam(discriminator.parameters(), lr=0.002)\n",
    "\n",
    "# Loss functions\n",
    "BCE_loss = nn.BCELoss()\n",
    "if cuda:\n",
    "    BCE_loss = BCE_loss.cuda()\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_disc_loss = 0\n",
    "    total_gen_loss = 0\n",
    "    total_rec_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        batch_size = data.size(0)\n",
    "        real = Variable(torch.ones(batch_size, 1))\n",
    "        fake = Variable(torch.zeros(batch_size, 1))\n",
    "\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "            real = real.cuda()\n",
    "            fake = fake.cuda()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_Disc.zero_grad()\n",
    "        \n",
    "        # Real latent vectors\n",
    "        z_real = Variable(torch.randn(batch_size, 20))\n",
    "        if cuda:\n",
    "            z_real = z_real.cuda()\n",
    "            \n",
    "        # Generate fake latent vectors\n",
    "        mu, logvar = encoder(data)\n",
    "        z_fake = encoder.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Discriminator losses\n",
    "        d_real = discriminator(z_real)\n",
    "        d_fake = discriminator(z_fake.detach())\n",
    "        d_loss_real = BCE_loss(d_real, real)\n",
    "        d_loss_fake = BCE_loss(d_fake, fake)\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_Disc.step()\n",
    "\n",
    "        # Train Generator (Encoder)\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Generate fake latent vectors\n",
    "        mu, logvar = encoder(data)\n",
    "        z_fake = encoder.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon = decoder(z_fake)\n",
    "        \n",
    "        # Generator loss (fool discriminator)\n",
    "        g_loss = BCE_loss(discriminator(z_fake), real)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        rec_loss = BCE_loss(recon, data)\n",
    "        \n",
    "        # Total generator loss\n",
    "        total_loss = rec_loss + 0.001 * g_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer_E.step()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Record losses\n",
    "        total_disc_loss += d_loss.item()\n",
    "        total_gen_loss += g_loss.item()\n",
    "        total_rec_loss += rec_loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch} [{batch_idx}/{len(train_loader)}] '\n",
    "                  f'D_loss: {d_loss.item():.4f} '\n",
    "                  f'G_loss: {g_loss.item():.4f} '\n",
    "                  f'Rec_loss: {rec_loss.item():.4f}')\n",
    "\n",
    "    return total_disc_loss/len(train_loader), total_gen_loss/len(train_loader), total_rec_loss/len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/250] D_loss: 150.6266 G_loss: 161.0388 Rec_loss: 110748.4688\n",
      "Epoch: 0 [100/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100369.6406\n",
      "Epoch: 0 [200/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100313.1562\n",
      "Epoch 0: Validation Loss: 100674.8013\n",
      "Epoch: 1 [0/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100980.4297\n",
      "Epoch: 1 [100/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100407.6094\n",
      "Epoch: 1 [200/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100480.5312\n",
      "Epoch 1: Validation Loss: 100674.7434\n",
      "Epoch: 2 [0/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100658.8281\n",
      "Epoch: 2 [100/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100542.5469\n",
      "Epoch: 2 [200/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 100779.9766\n",
      "Epoch 2: Validation Loss: 100674.7337\n",
      "Epoch: 3 [0/250] D_loss: 100.6409 G_loss: 62.6523 Rec_loss: 101072.3594\n",
      "Epoch: 3 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 3 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 3: Validation Loss: nan\n",
      "Epoch: 4 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 4 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 4 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 4: Validation Loss: nan\n",
      "Epoch: 5 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 5 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 5 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 5: Validation Loss: nan\n",
      "Epoch: 6 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 6 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 6 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 6: Validation Loss: nan\n",
      "Epoch: 7 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 7 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 7 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 7: Validation Loss: nan\n",
      "Epoch: 8 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 8 [100/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch: 8 [200/250] D_loss: nan G_loss: nan Rec_loss: nan\n",
      "Epoch 8: Validation Loss: nan\n",
      "Epoch: 9 [0/250] D_loss: nan G_loss: nan Rec_loss: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(loader):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            \n",
    "            mu, logvar = encoder(data)\n",
    "            z = encoder.reparameterize(mu, logvar)\n",
    "            recon = decoder(z)\n",
    "            \n",
    "            loss = bce_loss(recon, data)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(opt.n_epochs):\n",
    "    train_losses = train_epoch(epoch)\n",
    "    val_loss = evaluate(dev_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch}: Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
